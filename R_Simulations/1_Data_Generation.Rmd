---
title: 'Step 1: Data + Survival generation'
author: "Daniel Gomon"
date: '2023-10-23'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Global simulation parameters


```{r}
set.seed(1)
N <- 1600
p = 3
M = 6
times = seq(0, 15, 0.25) #Times at which longitudinal outcomes are sampled.
age = runif(N, min = 40, max = 90)


argvals_sim <- vector(mode = "list", length = p)
for(i in 1:p){
  argvals_sim[[i]] <- times
}

```

Sample observations in $[0,15]$ at quarter year intervals.
Age sampled uniformly between $40$ and $90$, making the oldest person in the data $105$ years old.
Six principal multivariate components (M).
Three (p) longitudinal variables.
1600 (N) patients.

```{r}
#library(devtools)
#Please install the development package from github (otherwise code won't run):
#install_github("d-gomon/MFPCA", ref = "b082ac2dd19be4027e51c49dec34ac2d745eaa65")
library(MFPCA)
library(RColorBrewer)
```





# Generate data


We generate realizations $X_i(\bm{t})$ from a $p(=3)$ dimensional process $X(\bm{t})$, where:
\begin{align*}
X_i^{(q)}(\bm{t}) = \sum_{m=1}^{M(=6)} \rho_{i,m} \psi_m(\bm{t})
\end{align*} Consider the Fourier functions on $[0,45]$:
\begin{align*}
    f_0(t) = \frac{1}{|\mathcal{T}|} &&
    f_{2r-1}(t) = \frac{\sin(rwt)}{\sqrt{\frac{|\mathcal{T}|}{2}}} &&
    f_{2r}(t) = \frac{\cos(rwt)}{\sqrt{\frac{|\mathcal{T}|}{2}}}
\end{align*} with $w = 2\pi/|\mathcal{T}|$ Take the first $M = 6$ fourier functions $f_0, f_1, ..., f_5$ on $[0, 15 \cdot p] = [0, 45]$. Let $T_1 = 0, T_2 = 15, T_3 = 30, T_4 = 45$ be split points and $\eta_1 = 0, \eta_2 = 15, \eta_3 = 30$. Then $\mathcal{T}_q = [T_q - \eta_q, T_{q+1} - \eta_{q}] = [0, 15]$ for $q = 1, 2, 3$. Randomly generate signs $\sigma_q \in \{-1, 1\}$ and obtain the eigenfunctions:

\begin{align*}
\psi_m^{(q)} = \sigma_q \left. f_m \right|_{[T_q, T_{q+1}]} (t_q - \eta_q)
\end{align*} The scores are then generated from a normal distribution: $\rho_{i,m} \sim \mathcal{N}(0, \nu_m)$ with $\nu_m = 65, 40, 18, 8, 7, 6$. The eigenvalues $\nu_m$ were determined on the ADNI data set decomposition:



```{r}
library(funData)

#Evaluate K-th fourier function on interval Tdomain at times t
#Tdomain has to be length 2 vector with lower and upper bound
fbasisfct <- function(t, Tdomain, K){
  #Get the K-th fourier basis function
  absT <- abs(Tdomain[2] - Tdomain[1])
  w = 2*pi/absT
  if(K == 0){
    return(rep(1/absT, length(t)))
  } else if(K %% 2 == 0){
    r = K/2
    return(cos(r * w * t)/sqrt(absT/2))
  } else if(K %% 2 == 1){
    r = (K+1)/2
    return(sin(r * w * t)/sqrt(absT/2))
  }
}

#Sample signs for data generation (step can be skipped in principle)
sigma_signs <- sample(c(-1, 1), size = p, replace = TRUE)

#Split points
T_split <- c(0, 15, 30, 45)
eta_split <- c(0, 15, 30)
Tdomain = c(0, 45)

#m-th eigenfunction at times t in dimension q
psi_fct <- function(t, q, m){
  return(sigma_signs[q] * fbasisfct(t + T_split[q], Tdomain = Tdomain, K = m))
}

eValsigma <- eVal(M = M, type = "linear")
library(MASS)
#Generate eigenvalues for data simulation
eVals <- mvrnorm(n = N, mu = c(0, 0, 0, 0, 0, 0), Sigma = diag(eValsigma))

#Function to obtain longitudinal trajectories for patients
Xt <- function(t, i, q){
  if(length(t) == 1){
    sum(eVals[i,] * sapply(0:5, function(k) psi_fct(t, q, m = k)))  
  } else{
    as.numeric(eVals[i,] %*% t(sapply(0:5, function(k) psi_fct(t, q = q, m = k))))
  }
}
```

Let us visualise our eigenfunctions, these should be either sines or cosines but then split in their domain and possibly reflected in the x-axis when we alternate over the different dimensions q:

```{r}
#Visualize psi^{(q)}_1
par(mfrow = c(1,3))
curve(psi_fct(x, q = 1, m = 1), 0, 15)
curve(psi_fct(x, q = 2, m = 1), 0, 15)
curve(psi_fct(x, q = 3, m = 1), 0, 15)
```
The difference between m=0, m=1 and m=2 should be that we see either a constant, a sine or a cosine (possibly mirrored in x-axis):
```{r}
#Visualize psi^{(q)}_1
par(mfrow = c(1,3))
curve(psi_fct(x, q = 1, m = 0), 0, 15)
curve(psi_fct(x, q = 1, m = 1), 0, 15)
curve(psi_fct(x, q = 1, m = 2), 0, 15)
```

Let us take a look at the resulting curve for a single individual for all 3 longitudinal variables:

```{r}
#Visualize longitudinal pattern in q = 1 dimension for subject 1
par(mfrow = c(1,3))
curve(Xt(x, i = 1, q = 1), 0, 15)
curve(Xt(x, i = 1, q = 2), 0, 15)
curve(Xt(x, i = 1, q = 3), 0, 15)

```
We can see that these indeed look like scaled sums of constants, sines and cosines.



## Generate the multiFunData

```{r}

Xt_q1 = t(sapply(1:N, function(i) Xt(times, i = i, q = 1)))
Xt_q2 = t(sapply(1:N, function(i) Xt(times, i = i, q = 2)))
Xt_q3 = t(sapply(1:N, function(i) Xt(times, i = i, q = 3)))

X1 <- funData(argvals = times, Xt_q1)
X2 <- funData(argvals = times, Xt_q2)
X3 <- funData(argvals = times, Xt_q3)


mFdat_noerror <- multiFunData(list(X1, X2, X3))

mFdat <- addError(mFdat_noerror, sd = 0.1)

```


## Mean function for study time

First we need to choose p mean functions and add them to the data.
Let's choose 3 functions which make sense on [0,15]
Function 1: $f(x) = 20 - (x/3-3)^2$
Function 2: $f(x) = \ln(x+1) - 10$
Function 3: $f(x) = exp(-(x-10)/5) + 5$

```{r}
fct_1 <- function(x) 20 - ((x-7.5)/1.2)^2
fct_2 <- function(x) log((x+1)^6) - 10
fct_3 <- function(x) exp(-(x-10)/5) + 2

curve(fct_1, from = 0, to = 15, ylim = c(-20, 20), xlab = "Time since baseline", ylab = "Value", main = "Mean function in observation time")
curve(fct_2, add = TRUE, col = "red")
curve(fct_3, add = TRUE, col = "green")

mean_1 <- funData(argvals = times, X = rbind(sapply(times, fct_1)))
mean_2 <- funData(argvals = times, X = rbind(sapply(times, fct_2)))
mean_3 <- funData(argvals = times, X = rbind(sapply(times, fct_3)))


mean_fct <- multiFunData(list(mean_1, mean_2, mean_3))

```



## Mean function for age

```{r}
cage_fct_1 <- function(x, age) 20-(((age+x)-73)/5)^2
cage_fct_2 <- function(x, age) log(((age+x)-40)^6) - 10
cage_fct_3 <- function(x, age) exp(-((age+x)-40)/20) + 5

curve(cage_fct_1(x, age = 40), from = 0, to = 65, main = "Mean function at age", xlab = "Age at observation - 40", ylim = c(-30, 20), ylab = "Value")
curve(cage_fct_2(x, age = 40), add = TRUE, col = "red")
curve(cage_fct_3(x, age = 40), add = TRUE, col = "green")
legend("bottomright", legend = c("q = 1", "q = 2", "q = 3"), col = c("black", "red", "green"), lty = 1)

mean_cage_1 <- funData(argvals = times, X = rbind(sapply(times, cage_fct_1, age = age)))
mean_cage_2 <- funData(argvals = times, X = rbind(sapply(times, cage_fct_2, age = age)))
mean_cage_3 <- funData(argvals = times, X = rbind(sapply(times, cage_fct_3, age = age)))

mean_fct_cage <- multiFunData(list(mean_cage_1, mean_cage_2, mean_cage_3))

```



## Create Age adjusted data as well

```{r}
mFdat_AA <- mFdat + mean_fct_cage
mFdat_norm <- mFdat + mean_fct
```


# Generate survival times

Let the hazard be defined by:
$$h_i(t) = h_0(t) \exp \left(   \sum_{q=1}^3 \alpha^{(q)} \eta_i^{(q)}(t) \right)$$
with $\alpha = [0.1, -0.1, 0.2]$ and 
$$\eta_i^{(q)}(t) = \sum_{m=1}^M \rho_{im} \psi^{(q)}_m(t)$$
. Generate survival probability $S_i(T_i)$ from uniform distribution and solve for patient specific survival time by solving $S_i(T_i) = \exp \left(- \int_0^{T_i} h_i(t) dt   \right)$ for $T_i$.  Use exponential baseline hazard with $\lambda = \exp(-3)$.

```{r}
#First simulate survival probabilities from standard uniform distribution:
surv_stdunif <- runif(N)


#Function to calculate h_i(t)
hit_func <- function(t, i, alpha, lambda){
  if(length(t) == 1){
    covs <- sum(alpha * sapply(1:3, function(x) Xt(t = t, i = i, q = x)))
  } else{
    covs <- as.numeric(alpha %*% t(sapply(1:3, function(x) Xt(t = t, i = i, q = x))))
  }
  exp(-lambda) * exp(covs)
}

#Function to calculate h_i(t)
library(eha)
hit_func_lognormal <- function(t, i, alpha, meanlog, sdlog){
  if(length(t) == 1){
    covs <- sum(alpha * sapply(1:3, function(x) Xt(t = t, i = i, q = x)))  
  } else{
    covs <- as.numeric(alpha %*% t(sapply(1:3, function(x) Xt(t = t, i = i, q = x))))
  }
  hlnorm(t, meanlog = meanlog, sdlog = sdlog) * exp(covs)
}

hit_func_weibull <- function(t, i, alpha, shape, scale){
  if(length(t) == 1){
    covs <- sum(alpha * sapply(1:3, function(x) Xt(t = t, i = i, q = x)))  
  } else{
    covs <- as.numeric(alpha %*% t(sapply(1:3, function(x) Xt(t = t, i = i, q = x))))
  }
  hweibull(x = t, shape = shape, scale = scale) * exp(covs)
}


#Survival function
Sti_func <- function(t, i, alpha, lambda){
  exp(-1*integrate(function(k) hit_func(t = k, i = i, alpha, lambda), lower = 0, upper = t)$value)
}

Sti_func_lognormal <- function(t, i, alpha, meanlog, sdlog){
  exp(-1*integrate(function(k) hit_func_lognormal(t = k, i = i, alpha, meanlog, sdlog), lower = 0, upper = t)$value)
}

Sti_func_weibull <- function(t, i, alpha, shape, scale){
  exp(-1*integrate(function(k) hit_func_weibull(t = k, i = i, alpha, shape, scale), lower = 0, upper = t)$value)
}


#sim_surv_exp <- sapply(1:N, function(l) {uniroot(function(t) Sti_func(t, i = l, alpha = c(1, -1, 2), lambda = 1.5) - surv_stdunif[l], lower = 0, upper = 200)$root})

sim_surv <- sapply(1:N, function(l) {uniroot(function(t) Sti_func_weibull(t, i = l, alpha = c(1, -1, 2), shape = 3, scale = 8.4) - surv_stdunif[l], lower = 0, upper = 200)$root})



#Admin censoring percentage:
sapply(list(sim_surv), function(x) 1- sum(x < 15)/length(x))

plot(density(sim_surv))
```


```{r}
cens_none <- rep(15, 1600)
cens_light <- pmax(pmin(rexp(1600, rate = exp(-3.5)), 15), 0.76)
cens_median <- pmax(pmin(rexp(1600, rate = exp(-2.75)), 15), 0.76)
cens_heavy <- pmax(pmin(rexp(1600, rate = exp(-2)), 15), 0.76)


censor_surv_times <- function(censor_times, surv_times){
  Y_out <- data.frame(
    time = pmin(censor_times, surv_times),
    status = as.numeric(surv_times <= censor_times)
  )
  return(as.matrix(Y_out))
}

#Generate censored survival times
Y_surv_sim_none <- censor_surv_times(cens_none, sim_surv)
Y_surv_sim_light <- censor_surv_times(cens_light, sim_surv)
Y_surv_sim_median <- censor_surv_times(cens_median, sim_surv)
Y_surv_sim_heavy <- censor_surv_times(cens_heavy, sim_surv)


hist(Y_surv_sim_light[which(Y_surv_sim_light[, "status"] == 1), "time"], breaks = 50)
hist(Y_surv_sim_median[which(Y_surv_sim_median[, "status"] == 1), "time"], breaks = 50)
hist(Y_surv_sim_heavy[which(Y_surv_sim_heavy[, "status"] == 1), "time"], breaks = 50)


#Percentages of censored observations:
#Want: ~0.20, 0.40, 0.60
lapply(list(Y_surv_sim_light, Y_surv_sim_median, Y_surv_sim_heavy), function(x) 1- sum(x[,2])/nrow(x))
```

```{r}
#Number of people at risk, each quarter year after 3 years.
numrisk_and_age <- function(Y_surv, times, age){
  event_times <- Y_surv[,1]
  ind_risk <- sapply(times, function(x) which(event_times >= x))
  num_risk <- sapply(times, function(x) sum(event_times >= x))
  age_dist <- lapply(1:length(times), function(x) age[ind_risk[[x]]] + times[x])
  names(age_dist) <- times
  return(list(num_risk = num_risk,
              ind_risk = ind_risk,
              age_dist = age_dist,
              times = times))
}
taims <- seq(0, 14.75, 0.25)

summary_none <- numrisk_and_age(Y_surv_sim_none, times = taims, age = age)
summary_light <- numrisk_and_age(Y_surv_sim_light, times = taims, age = age)
summary_median <- numrisk_and_age(Y_surv_sim_median, times = taims, age = age)
summary_heavy <- numrisk_and_age(Y_surv_sim_heavy, times = taims, age = age)
```


## Remove unobserved data from simulated data

```{r}
remove_unobserved_data <- function(mFData, Y_surv){
  
  #For each patient, we want to set all values after the event/censoring time to NA.
  #We loop over the data for each patient and do this
  
  Q <- length(mFData)
  N <- nObs(mFData)
  times <- mFData[[1]]@argvals[[1]]
  t_len <- length(times)
  
  #Starting from what index of times are observations censored for each individual?
  censor_idx <- sapply(1:nrow(Y_surv), function(x) which.min(Y_surv[x,1] >= times))
  
  for(i in 1:N){
    for(j in 1:Q){
      mFData[[j]]@X[i,censor_idx[i]:t_len] <- NA
    }
  }
  return(mFData)
}
```

```{r}
mFdat_norm_none <- remove_unobserved_data(mFdat_norm, Y_surv = Y_surv_sim_none)
mFdat_norm_light <- remove_unobserved_data(mFdat_norm, Y_surv = Y_surv_sim_light)
mFdat_norm_median <- remove_unobserved_data(mFdat_norm, Y_surv = Y_surv_sim_median)
mFdat_norm_heavy <- remove_unobserved_data(mFdat_norm, Y_surv = Y_surv_sim_heavy)

mFdat_AA_none <- remove_unobserved_data(mFdat_norm, Y_surv = Y_surv_sim_none)
mFdat_AA_light <- remove_unobserved_data(mFdat_AA, Y_surv = Y_surv_sim_light)
mFdat_AA_median <- remove_unobserved_data(mFdat_AA, Y_surv = Y_surv_sim_median)
mFdat_AA_heavy <- remove_unobserved_data(mFdat_AA, Y_surv = Y_surv_sim_heavy)
```

## Determine true Survival probabilities and scores

```{r}
truecdf <- matrix(NA, nrow = nObs(mFdat_norm), ncol = length(times))

for(i in 1:nObs(mFdat_norm)){
  truecdf[i,] <- 1 - sapply(times, function(t) Sti_func_weibull(t, i = i, alpha = c(1, -1, 2), shape = 3, scale = 8.4))
}

#Colnames important! Required to use true_validation
colnames(truecdf) <- times


#No censoring
trueScores_3_none <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_none, landmark_time = 3)
trueScores_6_none <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_none, landmark_time = 6)
trueScores_9_none <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_none, landmark_time = 9)

#Light censoring
trueScores_3_light <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_light, landmark_time = 3)
trueScores_6_light <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_light, landmark_time = 6)
trueScores_9_light <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_light, landmark_time = 9)

#Median censoring
trueScores_3_median <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_median, landmark_time = 3)
trueScores_6_median <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_median, landmark_time = 6)
trueScores_9_median <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_median, landmark_time = 9)

#Heavy censoring
trueScores_3_heavy <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_heavy, landmark_time = 3)
trueScores_6_heavy <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_heavy, landmark_time = 6)
trueScores_9_heavy <- true_validation(trueCDF = truecdf, Y_surv = Y_surv_sim_heavy, landmark_time = 9)


trueScores_3_none$MSE <- trueScores_3_light$MSE <- trueScores_3_median$MSE <- trueScores_3_heavy$MSE <- rep(0, length(trueScores_3_light$AUC_pred))
names(trueScores_3_none$MSE) <- names(trueScores_3_light$MSE) <- names(trueScores_3_median$MSE) <- names(trueScores_3_heavy$MSE) <- names(trueScores_3_light$AUC_pred)
trueScores_6_none$MSE <- trueScores_6_light$MSE <- trueScores_6_median$MSE <- trueScores_6_heavy$MSE <- rep(0, length(trueScores_6_light$AUC_pred))
names(trueScores_6_none$MSE) <- names(trueScores_6_light$MSE) <- names(trueScores_6_median$MSE) <- names(trueScores_6_heavy$MSE) <- names(trueScores_6_light$AUC_pred)
trueScores_9_none$MSE <- trueScores_9_light$MSE <- trueScores_9_median$MSE <- trueScores_9_heavy$MSE <- rep(0, length(trueScores_9_light$AUC_pred))
names(trueScores_9_none$MSE) <- names(trueScores_9_light$MSE) <- names(trueScores_9_median$MSE) <- names(trueScores_9_heavy$MSE) <- names(trueScores_9_light$AUC_pred)


```






## Save intermediary output

```{r}
save(mFdat_norm, mFdat_AA, mFdat_norm_none, mFdat_norm_light, mFdat_norm_median, mFdat_norm_heavy, mFdat_AA_none, mFdat_AA_light, mFdat_AA_median, mFdat_AA_heavy, age, summary_none, summary_light, summary_median, summary_heavy, trueScores_3_none, trueScores_3_light, trueScores_3_median, trueScores_3_heavy, trueScores_6_none, trueScores_6_light, trueScores_6_median, trueScores_6_heavy, trueScores_9_none, trueScores_9_light, trueScores_9_median, trueScores_9_heavy, truecdf, Y_surv_sim_none, Y_surv_sim_light, Y_surv_sim_median, Y_surv_sim_heavy, file = "SimulationData.Rdata")
```